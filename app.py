import os
import time
import ssl
import threading
from enum import Enum
from collections import deque

import cv2
import numpy as np
import sounddevice as sd
import soundfile as sf
import torch
import torchvision.transforms as transforms
from PIL import Image
import mediapipe as mp
import streamlit as st
from facenet_pytorch import InceptionResnetV1
from playsound3 import playsound
import gc

from services.rag.conversation_summarizer import summarize_and_store
from services.rag.rag_connecter import get_rag_response

from sqlalchemy.orm import Session
from db.session import SessionLocal
from db.models import User
import logging

# ----- logging setup & toggle -----
log = logging.getLogger("realtime-tts")


def configure_tts_logging(enabled: bool = True, level: int = logging.INFO) -> None:
    """
    ì‹¤ì‹œê°„ TTS ë¡œê¹… ì¼œê¸°/ë„ê¸°.
    - enabled=False: í•´ë‹¹ ë¡œê±°ì˜ ëª¨ë“  ë¡œê·¸ ë¹„í™œì„±í™”
    - enabled=True : levelë¡œ í™œì„±í™”
    """
    log.disabled = not enabled
    if enabled:
        log.setLevel(level)
        if not log.handlers:  # ì¤‘ë³µ í•¸ë“¤ëŸ¬ ë°©ì§€
            h = logging.StreamHandler()
            h.setFormatter(logging.Formatter(
                    "%(asctime)s.%(msecs)03d [%(levelname)s] %(message)s", "%H:%M:%S"
            ))
            log.addHandler(h)


# ê¸°ë³¸ê°’: ì¼œë‘  (ì›í•˜ë©´ ì•„ë˜ë¥¼ Falseë¡œ ë°”ê¾¸ì„¸ìš”)
configure_tts_logging(True)


def get_user_id_by_name(name: str) -> int | None:
    """ì£¼ì–´ì§„ nameì— í•´ë‹¹í•˜ëŠ” user_id ë°˜í™˜ (ì—†ìœ¼ë©´ None)"""
    with SessionLocal() as session:
        user = session.query(User).filter(User.name == name).first()
        return user.user_id if user else None


cv2.setNumThreads(1)  # â† ì„ íƒ: OpenCV ë‚´ë¶€ ìŠ¤ë ˆë“œ ê³¼ë‹¤ ì‚¬ìš© ì–µì œ

# ===== paths / dirs =====
ssl._create_default_https_context = ssl._create_unverified_context  # torch.hub SSL íšŒí”¼
TEMP_AUDIO_DIR = "audio"
os.makedirs(TEMP_AUDIO_DIR, exist_ok=True)

# ===== feature toggles =====
ENABLE_TTS = True  # Kokoroê°€ ì„¤ì¹˜ë¼ ìˆì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ skip
DEFAULT_TTS_LANG = "a"  # Kokoro lang_code
DEFAULT_TTS_VOICE = "af_heart"  # Kokoro voice (ì˜ˆ: 'af_heart')
DEFAULT_TTS_SR = 24000  # Kokoro sample rate


# =========================
# ìºì‹œëœ ì‹±ê¸€í†¤ ë¦¬ì†ŒìŠ¤ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
# =========================
@st.cache_resource
def get_whisper_model(model_name="medium.en", device=None):
    import whisper
    return whisper.load_model(model_name) if device is None else whisper.load_model(model_name, device=device)


@st.cache_resource
def get_facenet_model():
    return InceptionResnetV1(pretrained='vggface2').eval()


@st.cache_resource
def get_silero_vad_bundle():
    model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            trust_repo=True
    )
    return model, utils


@st.cache_resource
def get_face_detector():
    return mp.solutions.face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)


@st.cache_resource
def get_kokoro_pipeline():
    if not ENABLE_TTS:
        return None
    try:
        from kokoro import KPipeline
        pipeline = KPipeline(lang_code=DEFAULT_TTS_LANG)
        return pipeline
    except Exception as e:
        print(f"[TTS] Kokoro pipeline init failed: {e}")
        return None


# =========================
# VAD Recorder (shared model)
# =========================
class VADRecorder:
    def __init__(self, model=None, utils=None):
        if model is None or utils is None:
            model, utils = get_silero_vad_bundle()
        self.model = model
        self.utils = utils
        self.vad_iterator = self.utils[3](self.model)  # VADIterator

        self.SAMPLE_RATE = 16000
        self.BUFFER_SIZE = self.SAMPLE_RATE * 60  # 1 minute buffer
        self.THRESHOLD = 0.6
        self.MIN_DURATION = 0.5
        self.MARGIN = 1
        self.SILENCE_TIME = 1

        self.reset_state()

    def reset_state(self):
        self.audio_buffer = deque(maxlen=self.BUFFER_SIZE)
        self.is_speaking = False
        self.speech_start_sample = None
        self.sample_counter = 0
        self.silence_counter = 0
        self.ema_speech_prob = 0
        self.saved_filename = None

    def _save_audio_segment(self, start_sample, end_sample):
        audio_array = np.array(list(self.audio_buffer), dtype=np.int16)
        start = max(0, start_sample - int(self.MARGIN * self.SAMPLE_RATE))
        end = min(len(audio_array), end_sample + int(self.MARGIN * self.SAMPLE_RATE))
        segment = audio_array[start:end]
        if len(segment) / self.SAMPLE_RATE < self.MIN_DURATION:
            print("Segment too short, skipping save.")
            return
        filename = f"{TEMP_AUDIO_DIR}/speech_{time.strftime('%Y%m%d_%H%M%S')}.wav"
        sf.write(filename, segment, self.SAMPLE_RATE)
        print(f"Audio saved: {filename}")
        self.saved_filename = filename

    def _callback(self, indata, frames, time_info, status):
        if status:
            print(status)
        if self.saved_filename:
            return

        audio_int16 = (indata * 32768).astype(np.int16).flatten()
        self.audio_buffer.extend(audio_int16)
        # if len(audio_int16) < 512:
        # return

        audio_tensor = torch.from_numpy(audio_int16).float() * 0.9
        speech_prob = self.vad_iterator.model(audio_tensor, self.SAMPLE_RATE).item()
        self.ema_speech_prob = 0.9 * self.ema_speech_prob + 0.1 * speech_prob

        if self.ema_speech_prob > self.THRESHOLD:
            if not self.is_speaking:
                self.is_speaking = True
                self.speech_start_sample = self.sample_counter
            self.silence_counter = 0
        else:
            if self.is_speaking:
                self.silence_counter += frames / self.SAMPLE_RATE
                if self.silence_counter >= self.SILENCE_TIME:
                    self.is_speaking = False
                    speech_end_sample = self.sample_counter
                    duration = (speech_end_sample - self.speech_start_sample) / self.SAMPLE_RATE
                    if duration >= self.MIN_DURATION:
                        self._save_audio_segment(self.speech_start_sample, speech_end_sample)
        self.sample_counter += frames

    def record(self, timeout=10):
        self.reset_state()
        stream = sd.InputStream(callback=self._callback, channels=1, samplerate=self.SAMPLE_RATE, blocksize=512)
        with stream:
            print("Listening for speech...")
            start_time = time.time()
            while time.time() - start_time < timeout:
                if self.saved_filename:
                    break
                sd.sleep(100)
        print("Finished listening.")
        return self.saved_filename


def listen_and_record_speech(timeout=10, model=None, utils=None):
    if model is None or utils is None:
        raise RuntimeError("VAD model/utils must be provided from main thread")
    recorder = VADRecorder(model=model, utils=utils)
    return recorder.record(timeout=timeout)


# =========================
# State Definition
# =========================
class State(Enum):
    IDLE = 0
    USER_CHECK = 1
    ENROLL = 2
    WELCOME = 3
    ASR = 4
    BYE = 5


# =========================
# Globals & Flags
# =========================
FACE_DETECTED = False
USER_EXIST = False
ENROLL_SUCCESS = False
VAD = False
BYE_EXIST = False
TIMER_EXPIRED = False  # WELCOME/BYE timer

# shared data
sh_face_crop = None
sh_bbox = None
sh_embedding = None
sh_current_user = None
sh_audio_file = None
sh_tts_file = None
sh_message = "Initializing..."
sh_color = (255, 255, 0)
sh_timer_end = 0
# sh_prev_unkown = None

SESSION_USER = None
USER_SWITCHED = False

# â— ì„¸ì…˜ í•œì • ê·¸ë£¹ëª… (WELCOME~BYE ì‚¬ì´ ë©”ëª¨ë¦¬ ë³´ê´€)
sh_session_group = None

# async flags
VAD_TASK_STARTED = False
VAD_TASK_RUNNING = False
ASR_TASK_STARTED = False
ASR_TASK_RUNNING = False
ASR_TEXT = None

sh_transcript = []

# DB / threshold
DB_PATH = "faces_db.npy"
SIM_THRESHOLD = 0.65

# BBOX smoothing
BBOX_AVG_N = 5
_bbox_history = deque(maxlen=BBOX_AVG_N)


# =========================
# Utils (DB: name_list + embeddings ë§Œ ì‚¬ìš©)
# =========================
def load_db():
    if os.path.exists(DB_PATH):
        data = np.load(DB_PATH, allow_pickle=True).item()
        name = data["name_list"]
        embs = data["embeddings"]
        return name, embs
    else:
        return [], np.empty((0, 512))


def save_db(name_list, embeddings):
    np.save(DB_PATH, {"name_list": name_list, "embeddings": embeddings})


def find_match(embedding, name_list, embeddings):
    if len(embeddings) == 0:
        return None, 0
    sims = [np.dot(embedding, emb) / (np.linalg.norm(embedding) * np.linalg.norm(emb)) for emb in embeddings]
    max_idx = np.argmax(sims)
    if sims[max_idx] >= SIM_THRESHOLD:
        return name_list[max_idx], sims[max_idx]
    else:
        return None, sims[max_idx]


def detect_user_change():
    """ì„¸ì…˜ ì¤‘ ì‚¬ìš©ì ë³€ê²½ ê°ì§€: SESSION_USERì™€ í˜„ì¬ í”„ë ˆì„ì˜ ë§¤ì¹­ ê²°ê³¼ê°€ ë‹¤ë¥´ë©´ True"""
    global USER_SWITCHED, sh_message, sh_color

    if SESSION_USER is None:
        return  # ì•„ì§ ì„¸ì…˜ ì‹œì‘ ì „

    if sh_face_crop is None or sh_face_crop.size == 0:
        return  # ì–¼êµ´ ë¯¸ê²€ì¶œ ìƒíƒœëŠ” ë³€ê²½ìœ¼ë¡œ ë³´ì§€ ì•ŠìŒ

    face_pil = Image.fromarray(cv2.cvtColor(sh_face_crop, cv2.COLOR_BGR2RGB))
    face_tensor = preprocess(face_pil).unsqueeze(0)
    with torch.no_grad():
        emb = resnet(face_tensor)[0].cpu().numpy()
    emb = emb / np.linalg.norm(emb)

    match_name, sim = find_match(emb, name_list, embeddings)

    # ë‹¤ë¥¸ "ì•Œë ¤ì§„" ì‚¬ìš©ìê°€ ì¡íˆë©´ ë³€ê²½ìœ¼ë¡œ íŒë‹¨
    if match_name and match_name != SESSION_USER:
        USER_SWITCHED = True
        sh_message = f"User changed â†’ {match_name}. Ending session..."
        sh_color = (255, 0, 0)


def _clip_bbox(x, y, w, h, iw, ih):
    x = max(0, min(x, iw - 1))
    y = max(0, min(y, ih - 1))
    w = max(1, min(w, iw - x))
    h = max(1, min(h, ih - y))
    return x, y, w, h


# Face detection + averaged bbox over last N frames
def update_face_detection():
    global FACE_DETECTED, sh_face_crop, sh_bbox, sh_frame, _bbox_history, BBOX_AVG_N

    # deque maxlen ë™ê¸°í™”
    if _bbox_history.maxlen != BBOX_AVG_N:
        _bbox_history = deque(list(_bbox_history), maxlen=BBOX_AVG_N)

    image = sh_frame  # ë¶ˆí•„ìš”í•œ copy() ì‚­ì œ
    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = face_detection.process(rgb_image)

    ih, iw, _ = image.shape
    det_count = len(results.detections) if (results and results.detections) else 0

    if det_count == 1:
        FACE_DETECTED = True
        det = results.detections[0]
        bboxC = det.location_data.relative_bounding_box
        x = int(bboxC.xmin * iw);
        y = int(bboxC.ymin * ih)
        w = int(bboxC.width * iw);
        h = int(bboxC.height * ih)
        x, y, w, h = _clip_bbox(x, y, w, h, iw, ih)

        _bbox_history.append((x, y, w, h))
        xs = np.mean([b[0] for b in _bbox_history])
        ys = np.mean([b[1] for b in _bbox_history])
        ws = np.mean([b[2] for b in _bbox_history])
        hs = np.mean([b[3] for b in _bbox_history])
        xa, ya, wa, ha = _clip_bbox(int(xs), int(ys), int(ws), int(hs), iw, ih)

        sh_bbox = (xa, ya, wa, ha)
        # ROIëŠ” copy()ë¡œ ë¶„ë¦¬í•´ ìƒìœ„ í”„ë ˆì„ ë²„í¼ ì°¸ì¡° ëŠê¸°
        sh_face_crop = image[ya:ya + ha, xa:xa + wa].copy()
        if sh_face_crop.size == 0:
            FACE_DETECTED = False
            _bbox_history.clear()
            sh_bbox = None
            sh_face_crop = None
    else:
        FACE_DETECTED = False
        _bbox_history.clear()
        sh_bbox = None
        sh_face_crop = None

    # ë¬´ê±°ìš´ ì¤‘ê°„ ê°ì²´ ì¦‰ì‹œ í•´ì œ íŒíŠ¸
    del rgb_image, results
    return det_count


# =========================
# Whisper ASR (cached)
# =========================
def asr_from_wav(file_path: str) -> str:
    result = whisper_model.transcribe(file_path, language='en', fp16=False)
    return result['text']


import queue


# =========================
# Realtime Speech Orchestrator
# =========================
class RealtimeSpeechOrchestrator:
    """
    RAG(openai.Stream) í…ìŠ¤íŠ¸ ë¸íƒ€ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì˜ë¼ íì— ë„£ê³ ,
    ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ Kokoro í•©ì„± â†’ ì¦‰ì‹œ sounddevice.OutputStreamìœ¼ë¡œ ì¬ìƒ.
    íŒŒì¼ ì €ì¥ ì—†ìŒ.

    â˜…ì¶”ê°€: chunk ì „ë‹¬ í™•ì¸ìš© íŠ¸ë ˆì´ì‹±/ë©”íŠ¸ë¦­ìŠ¤ ë‚´ì¥
    - delta ìˆ˜ì‹ /flush/synth/play ê° ì‹œì  ë¡œê¹…
    - ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ ë ˆì´í„´ì‹œ(Flushâ†’Synth1, Flushâ†’Play1) ì¸¡ì •
    """

    def __init__(
            self,
            pipeline,
            sr: int = 24000,
            voice: str = "af_heart",
            sentence_enders=(".", "!", "?", "â€¦"),
            max_pending_chars: int = 80,
            blocksize: int = 1024,
    ):
        self.pipeline = pipeline
        self.sr = sr
        self.voice = voice
        self.sentence_enders = sentence_enders
        self.max_pending_chars = max_pending_chars
        self.blocksize = blocksize

        # í: (segment_id, text) / (segment_id, audio_chunk)
        self.text_queue: "queue.Queue[Tuple[int, str]]" = queue.Queue(maxsize=200)
        self.audio_queue: "queue.Queue[Tuple[Optional[int], Optional[np.ndarray]]]" = queue.Queue(maxsize=200)

        self.stop_text = threading.Event()
        self.stop_audio = threading.Event()
        self.kokoro_lock = threading.Lock()  # pipelineì´ ìŠ¤ë ˆë“œ-ì„¸ì´í”„ê°€ ì•„ë‹ ìˆ˜ ìˆì–´ ë³´í˜¸

        self.synth_t = None
        self.play_t = None

        self._full_text_chunks: list[str] = []
        self._t0 = time.perf_counter()

        # ---- tracing / metrics ----
        self.delta_count = 0
        self.flush_count = 0
        self.synth_chunk_count = 0
        self.play_chunk_count = 0

        self.seg_id_counter = 0
        self.seg_flush_time = {}  # seg_id -> t_flush
        self.seg_first_synth_time = {}  # seg_id -> t_first_audio_enqueued
        self.seg_first_play_time = {}  # seg_id -> t_first_audio_written

    # --- utils ---
    def _now(self) -> float:
        return time.perf_counter() - self._t0

    def _should_flush(self, s: str) -> bool:
        s = s.strip()
        if not s:
            return False
        return s.endswith(self.sentence_enders) or len(s) >= self.max_pending_chars

    # --- threads ---
    def start(self):
        # í•©ì„± ìŠ¤ë ˆë“œ
        def synth_worker():
            try:
                while not (self.stop_text.is_set() and self.text_queue.empty()):
                    try:
                        seg_id, seg = self.text_queue.get(timeout=0.05)
                    except queue.Empty:
                        continue
                    if not seg.strip():
                        continue

                    log.info(f"[SYNTH] seg#{seg_id} start (len={len(seg)})")
                    try:
                        with self.kokoro_lock:
                            first_chunk = True
                            for _, _, audio in self.pipeline(seg, voice=self.voice):
                                a = np.asarray(audio, dtype=np.float32)
                                # ì²« í•©ì„± ì‹œê° ê¸°ë¡
                                if first_chunk:
                                    first_chunk = False
                                    self.seg_first_synth_time.setdefault(seg_id, self._now())
                                    log.info(
                                            f"[SYNTH] seg#{seg_id} first-chunk "
                                            f"(samples={len(a)}, dur={len(a) / self.sr:.3f}s, "
                                            f"flushâ†’synth={self.seg_first_synth_time[seg_id] - self.seg_flush_time[seg_id]:.3f}s)"
                                    )

                                while True:
                                    try:
                                        self.audio_queue.put((seg_id, a), timeout=0.05)
                                        self.synth_chunk_count += 1
                                        break
                                    except queue.Full:
                                        if self.stop_audio.is_set():
                                            return
                    except Exception as e:
                        log.exception(f"[TTS] synth error: {e}")
                # í•©ì„± ì¢…ë£Œ ì‹ í˜¸
                self.audio_queue.put((None, None))
            except Exception as e:
                log.exception(f"[TTS] synth_worker crashed: {e}")

        # ì¬ìƒ ìŠ¤ë ˆë“œ
        def playback_worker():
            try:
                with sd.OutputStream(
                        samplerate=self.sr,
                        channels=1,
                        dtype="float32",
                        blocksize=self.blocksize,
                        latency="low",
                ) as out:
                    seen_first_play_for_seg = set()
                    while True:
                        try:
                            seg_id, chunk = self.audio_queue.get(timeout=0.1)
                        except queue.Empty:
                            if self.stop_audio.is_set():
                                break
                            continue
                        if seg_id is None and chunk is None:  # í•©ì„± ì¢…ë£Œ
                            break

                        # ì„¸ê·¸ë¨¼íŠ¸ ì²« ì¬ìƒì‹œê° ê¸°ë¡
                        if seg_id is not None and seg_id not in seen_first_play_for_seg:
                            seen_first_play_for_seg.add(seg_id)
                            self.seg_first_play_time.setdefault(seg_id, self._now())
                            # flushâ†’play ë ˆì´í„´ì‹œ
                            fp = self.seg_first_play_time[seg_id] - self.seg_flush_time[seg_id]
                            # synthâ†’play ë ˆì´í„´ì‹œ
                            sp = (
                                    self.seg_first_play_time[seg_id]
                                    - self.seg_first_synth_time.get(seg_id, self.seg_first_play_time[seg_id])
                            )
                            log.info(
                                    f"[PLAY ] seg#{seg_id} first-write "
                                    f"(flushâ†’play={fp:.3f}s, synthâ†’play={sp:.3f}s)"
                            )

                        out.write(chunk)
                        self.play_chunk_count += 1
            except Exception as e:
                log.exception(f"[Audio] playback error: {e}")

        self.synth_t = threading.Thread(target=synth_worker, daemon=True)
        self.play_t = threading.Thread(target=playback_worker, daemon=True)
        self.synth_t.start()
        self.play_t.start()

    # --- stream feed ---
    def feed_openai_stream(self, stream):
        """
        openai.Streamì„ ì½ì–´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ text_queueì— íˆ¬ì….
        """
        pending: list[str] = []
        for event in stream:
            et = getattr(event, "type", "")
            if et == "response.output_text.delta":
                delta = event.delta or ""
                if delta:
                    self.delta_count += 1
                    t = self._now()
                    log.info(
                            f"[DELTA] #{self.delta_count} @{t:.3f}s len={len(delta)} text='{delta[:40].replace(chr(10), ' ')}...'")
                    self._full_text_chunks.append(delta)
                    pending.append(delta)
                    seg = "".join(pending)
                    if self._should_flush(seg):
                        self.seg_id_counter += 1
                        seg_id = self.seg_id_counter
                        self.text_queue.put((seg_id, seg))
                        self.seg_flush_time[seg_id] = self._now()
                        self.flush_count += 1
                        log.info(f"[FLUSH] seg#{seg_id} @{self.seg_flush_time[seg_id]:.3f}s len={len(seg)}")
                        pending.clear()

            elif et == "response.completed":
                tail = "".join(pending).strip()
                if tail:
                    self.seg_id_counter += 1
                    seg_id = self.seg_id_counter
                    self.text_queue.put((seg_id, tail))
                    self.seg_flush_time[seg_id] = self._now()
                    self.flush_count += 1
                    log.info(f"[FLUSH] seg#{seg_id} (tail) @{self.seg_flush_time[seg_id]:.3f}s len={len(tail)}")
                break

            elif et == "response.error":
                log.error(f"[RAG] stream error: {getattr(event, 'error', None)}")
                break

            # ê·¸ ì™¸ ì´ë²¤íŠ¸ íƒ€ì…ì€ ë¬´ì‹œ
        self.stop_text.set()

    # --- teardown / report ---
    def wait(self):
        # í•©ì„± ì¢…ë£Œê¹Œì§€ ëŒ€ê¸°
        if self.synth_t:
            self.synth_t.join()
        # ì¬ìƒ ì¢…ë£Œ ì‹ í˜¸
        self.stop_audio.set()
        if self.play_t:
            self.play_t.join()

    def report(self):
        log.info("==== STREAMING REPORT ====")
        log.info(f"deltas={self.delta_count}, flushes={self.flush_count}, "
                 f"synth_chunks={self.synth_chunk_count}, play_chunks={self.play_chunk_count}")

        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë ˆì´í„´ì‹œ
        for seg_id in sorted(self.seg_flush_time.keys()):
            t_flush = self.seg_flush_time.get(seg_id)
            t_synth = self.seg_first_synth_time.get(seg_id)
            t_play = self.seg_first_play_time.get(seg_id)
            fs = (t_synth - t_flush) if (t_flush is not None and t_synth is not None) else None
            fp = (t_play - t_flush) if (t_flush is not None and t_play is not None) else None
            log.info(
                    f"seg#{seg_id}: flush={t_flush:.3f}s, "
                    f"first_synth={t_synth:.3f}s" if t_synth is not None else f"seg#{seg_id}: flush={t_flush:.3f}s, first_synth=None"
            )
            if fp is not None:
                log.info(f"        â†’ flushâ†’play = {fp:.3f}s"
                         + (f", flushâ†’synth = {fs:.3f}s" if fs is not None else ""))
        log.info("==========================")

    @property
    def full_text(self) -> str:
        return "".join(self._full_text_chunks)


# =========================
# TTS helpers (Kokoro)
# =========================
def build_tts_reply_text(asr_text: str, user: str | None) -> str:
    t = "".join(asr_text.split()).lower()
    if "ì˜ê°€" in t or "bye" in t:
        return f"Good bye, {user if user else ''}."
    # ê°„ë‹¨ ì—ì½” ì‘ë‹µ
    return f"{asr_text}"


def synthesize_tts_kokoro(text: str) -> str | None:
    # ë©”ì¸ì—ì„œ ë§Œë“  kokoro_pipeline ì „ì—­ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìŠ¤ë ˆë“œì—ì„œ cache í˜¸ì¶œ ê¸ˆì§€)
    if kokoro_pipeline is None or not ENABLE_TTS:
        return None
    try:
        chunks = []
        for _, _, audio in kokoro_pipeline(text, voice=DEFAULT_TTS_VOICE):
            chunks.append(audio)
        if not chunks:
            return None
        audio = chunks[0]
        out_path = f"{TEMP_AUDIO_DIR}/tts_{int(time.time())}.wav"
        sf.write(out_path, audio, DEFAULT_TTS_SR)
        playsound(out_path)
        return out_path
    except Exception as e:
        print(f"[TTS] synthesis failed: {e}")
        return None


# =========================
# State Action Functions
# =========================
def enter_idle():
    global sh_message, sh_color
    det_count = update_face_detection()
    if not FACE_DETECTED:
        if det_count > 1:
            sh_message = f"{det_count} faces detected. Only one please."
            sh_color = (0, 0, 255)
        else:
            sh_message = "Waiting for user..."
            sh_color = (255, 255, 0)


def enter_user_check():
    global USER_EXIST, sh_embedding, sh_current_user, sh_message, sh_color
    update_face_detection()
    if sh_face_crop is None:
        return
    face_pil = Image.fromarray(cv2.cvtColor(sh_face_crop, cv2.COLOR_BGR2RGB))
    face_tensor = preprocess(face_pil).unsqueeze(0)
    with torch.no_grad():
        embedding = resnet(face_tensor)[0].cpu().numpy()
    sh_embedding = embedding / np.linalg.norm(embedding)

    match_name, sim = find_match(sh_embedding, name_list, embeddings)
    if match_name:
        USER_EXIST = True
        sh_current_user = match_name
        sh_message = f"Identifying... {match_name} ({sim:.2f})"
        sh_color = (0, 255, 0)
    else:
        USER_EXIST = False
        sh_message = "Unknown user. Use the right panel to enroll."
        sh_color = (0, 255, 255)
    # gc.collect()


def enter_enroll(key=None):
    global sh_message, sh_color
    det_count = update_face_detection()
    if not FACE_DETECTED:
        if det_count > 1:
            sh_message = f"{det_count} faces detected. Only one please."
            sh_color = (0, 0, 255)
        else:
            sh_message = "ë“±ë¡ì„ ìœ„í•´ ì–¼êµ´ì„ ì¹´ë©”ë¼ì— ë¹„ì¶°ì£¼ì„¸ìš”."
            sh_color = (255, 255, 0)
    else:
        sh_message = "ì•Œ ìˆ˜ ì—†ëŠ” ì‚¬ìš©ìì…ë‹ˆë‹¤. ì˜¤ë¥¸ìª½ íŒ¨ë„ì˜ í¼ìœ¼ë¡œ ë“±ë¡í•˜ì„¸ìš”."
        sh_color = (0, 255, 255)


def enter_welcome():
    global VAD, sh_audio_file, TIMER_EXPIRED, sh_message, sh_color
    update_face_detection()
    detect_user_change()

    sh_message = f"Hi, {sh_current_user}!"
    sh_color = (0, 255, 0)

    TIMER_EXPIRED = (time.time() > sh_timer_end)
    if TIMER_EXPIRED and not VAD_TASK_STARTED:
        start_vad_async(timeout=5)


def enter_asr():
    update_face_detection()
    detect_user_change()

    if sh_audio_file and not ASR_TASK_STARTED:
        start_asr_async(sh_audio_file)


def enter_bye():
    global TIMER_EXPIRED, sh_message, sh_color
    update_face_detection()
    sh_message = f"Bye, {sh_current_user}!"
    sh_color = (255, 0, 255)
    TIMER_EXPIRED = (time.time() > sh_timer_end)


# =========================
# Async Workers
# =========================
def start_vad_async(timeout=5):
    """ë…¹ìŒì„ ë¹„ë™ê¸°ë¡œ ì‹œì‘."""
    global VAD_TASK_STARTED, VAD_TASK_RUNNING
    if VAD_TASK_RUNNING:
        return
    VAD_TASK_STARTED = True
    VAD_TASK_RUNNING = True

    def _worker():
        global sh_audio_file, VAD, VAD_TASK_RUNNING
        try:
            filename = listen_and_record_speech(timeout=timeout, model=vad_model, utils=vad_utils)
            if filename:
                sh_audio_file = filename
                VAD = True
            else:
                VAD = False
        finally:
            VAD_TASK_RUNNING = False

    threading.Thread(target=_worker, daemon=True).start()


def start_asr_async(file_path: str):
    """Whisper ë¹„ë™ê¸° + RAG â†’ ì‹¤ì‹œê°„ TTS(íŒŒì¼ ì €ì¥ ì—†ìŒ)."""
    global ASR_TASK_STARTED, ASR_TASK_RUNNING
    if ASR_TASK_RUNNING:
        return
    ASR_TASK_STARTED = True
    ASR_TASK_RUNNING = True

    def _worker():
        global ASR_TEXT, BYE_EXIST, ASR_TASK_RUNNING, sh_tts_file
        try:
            text = asr_from_wav(file_path)
            ASR_TEXT = text.encode("ascii", "ignore").decode("ascii").strip()
            print("[ASR] ", ASR_TEXT)
            t = "".join(ASR_TEXT.split()).lower()
            BYE_EXIST = ("ì˜ê°€" in t) or ("bye" in t)

            if BYE_EXIST:
                # ê°„ë‹¨í•œ ê³ ë³„ ë©˜íŠ¸ë§Œ ì¦‰ì‹œ í•©ì„±/ì¬ìƒ
                if kokoro_pipeline is not None and ENABLE_TTS:
                    orch = RealtimeSpeechOrchestrator(
                            pipeline=kokoro_pipeline,
                            sr=DEFAULT_TTS_SR,
                            voice=DEFAULT_TTS_VOICE
                    )
                    orch.start()
                    # BYE í…ìŠ¤íŠ¸ ë°”ë¡œ íˆ¬ì…
                    orch.text_queue.put(build_tts_reply_text(ASR_TEXT, sh_current_user))
                    orch.stop_text.set()
                    orch.wait()
                else:
                    print("[TTS] pipeline not available.")
                sh_tts_file = None  # íŒŒì¼ ì—†ìŒ
            else:
                # 1) RAG ìŠ¤íŠ¸ë¦¼ ì‹œì‘
                stream = get_rag_response(
                        user_id=get_user_id_by_name(SESSION_USER),
                        query=str(ASR_TEXT),
                        target="team",
                        group_name=sh_session_group,
                )
                # 2) ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê¸°ë™ â†’ ì¦‰ì‹œ í•©ì„±/ì¬ìƒ
                if kokoro_pipeline is not None and ENABLE_TTS:
                    orch = RealtimeSpeechOrchestrator(
                            pipeline=kokoro_pipeline,
                            sr=DEFAULT_TTS_SR,
                            voice=DEFAULT_TTS_VOICE
                    )
                    orch.start()
                    orch.feed_openai_stream(stream)  # ìŠ¤íŠ¸ë¦¼ ì½ìœ¼ë©° ë¬¸ì¥ íì— íˆ¬ì…
                    orch.wait()
                    answer = orch.full_text
                else:
                    # TTS ë¶ˆê°€ ì‹œ í…ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘
                    collected = []
                    for event in stream:
                        if getattr(event, "type", "") == "response.output_text.delta":
                            collected.append(event.delta or "")
                        elif getattr(event, "type", "") in ("response.completed", "response.error"):
                            break
                    answer = "".join(collected)

                # ëŒ€í™” ê¸°ë¡
                sh_transcript.append({"role": "user", "content": ASR_TEXT})
                sh_transcript.append({"role": "assistant", "content": answer})
                sh_tts_file = None  # íŒŒì¼ ì €ì¥í•˜ì§€ ì•ŠìŒ
        finally:
            ASR_TASK_RUNNING = False

    threading.Thread(target=_worker, daemon=True).start()


# =========================
# Transitions & Dispatcher
# =========================
def state_transition(current_state: State) -> State:
    global sh_embedding, name_list, embeddings  # ,sh_prev_unkown

    if current_state == State.IDLE:
        return State.USER_CHECK if FACE_DETECTED else State.IDLE

    elif current_state == State.USER_CHECK:
        if USER_EXIST:
            return State.WELCOME if sh_session_group else State.USER_CHECK
        else:
            return State.ENROLL

    elif current_state == State.ENROLL:
        if ENROLL_SUCCESS:
            name_list, embeddings = load_db()
            # USER_CHECKë¡œ ëŒì•„ê°€ group ì…ë ¥
            return State.USER_CHECK
        # sh_prev_unkown = sh_embedding
        return State.IDLE if not FACE_DETECTED else State.ENROLL

    elif current_state == State.WELCOME:
        if USER_SWITCHED:
            return State.BYE

        if not (time.time() > sh_timer_end):
            return State.WELCOME
        if VAD:
            return State.ASR
        if VAD_TASK_RUNNING:
            return State.WELCOME
        return State.IDLE

    elif current_state == State.ASR:
        if USER_SWITCHED:
            return State.BYE

        if ASR_TASK_RUNNING:
            return State.ASR
        if ASR_TASK_STARTED and not ASR_TASK_RUNNING:
            return State.BYE if BYE_EXIST else State.IDLE
        return State.ASR

    elif current_state == State.BYE:
        if TIMER_EXPIRED:
            # TODO ëŒ€í™” ìš”ì•½ ì €ì¥
            res = summarize_and_store(me_id=get_user_id_by_name(SESSION_USER), messages=sh_transcript,
                                      visibility="group")
            print(f"(conversation saved) text_ref={res['text_ref']} id={res['embedding_id']}")
            return State.IDLE

        return State.BYE

    return current_state


def call_state_fn(state: State, key):
    if state == State.IDLE:
        enter_idle()
    elif state == State.USER_CHECK:
        enter_user_check()
    elif state == State.ENROLL:
        enter_enroll(key)
    elif state == State.WELCOME:
        enter_welcome()
    elif state == State.ASR:
        enter_asr()
    elif state == State.BYE:
        enter_bye()


# =========================
# Model Init (cached)
# =========================
print("Loading models (cached)...")
resnet = get_facenet_model()
face_detection = get_face_detector()
whisper_model = get_whisper_model("base.en")

# ì—¬ê¸°ì„œ ë¯¸ë¦¬ ë¡œë“œí•˜ê³ , ì „ì—­ìœ¼ë¡œ ë“¤ê³ ë§Œ ìˆìŒ (ìŠ¤ë ˆë“œì—ì„œ ìƒˆë¡œ ë¶€ë¥´ì§€ ì•ŠìŒ)
vad_model, vad_utils = get_silero_vad_bundle()
kokoro_pipeline = get_kokoro_pipeline()

preprocess = transforms.Compose([
        transforms.Resize((160, 160)), transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])
name_list, embeddings = load_db()
print("Models loaded (using cache).")

# =========================
# Streamlit UI & Main Loop
# =========================
st.set_page_config(page_title="Face Kiosk", layout="wide")
st.title("Famigo AI")

col_video, col_ui = st.columns([3, 2], vertical_alignment="top")

# Camera / Options
with col_video:
    st.subheader("Camera")
    cam_index = st.number_input("Camera index", min_value=0, max_value=10, value=0, step=1)
    width = st.slider("Frame width", 320, 1920, 640, step=10)
    bbox_avg_n_ui = st.slider("BBOX smoothing (frames)", 1, 30, 5, help="Average the face bbox over N frames.")
    run = st.toggle("Run camera", value=True)
    frame_slot = st.empty()

# UI placeholders
with col_ui:
    st.subheader("Group")
    group_ui = st.empty()  # â† ê·¸ë£¹ ì…ë ¥ ì „ìš© placeholder
    st.subheader("State Panel")
    state_badge = st.empty()
    message_slot = st.empty()
    usercheck_slot = st.empty()
    enroll_slot = st.empty()
    welcome_slot = st.empty()
    asr_slot = st.empty()
    bye_slot = st.empty()
    audio_slot = st.empty()
    debug_slot = st.expander("Debug", expanded=False)
    with debug_slot:
        debug_ph = st.empty()  # â† í•œ ì¹¸ í™•ë³´

if "group_key_counter" not in st.session_state:
    st.session_state.group_key_counter = 1

if "current_group_key" not in st.session_state:
    st.session_state.current_group_key = f"usercheck_group_input_{st.session_state.group_key_counter}"

# ğŸ‘‡ ê·¸ë£¹ ì…ë ¥ì°½: ë£¨í”„ ë°”ê¹¥ì—ì„œ ë‹¨ 1íšŒë§Œ ìƒì„±
with group_ui.container():
    _gkey = st.session_state.current_group_key
    _gval = (st.session_state.get(_gkey, "") or "").strip()

    st.text_input(
            "ê·¸ë£¹ëª… (BYE í›„ì—ë§Œ ë‹¤ì‹œ ì…ë ¥)",
            key=_gkey,  # â† í˜„ì¬ í‚¤ë§Œ ì‚¬ìš©
            placeholder="ì˜ˆ: slpr",
            disabled=bool(_gval),  # ê°’ì´ ìˆìœ¼ë©´ ì ê¸ˆ
    )
    st.caption(f"í˜„ì¬ ê·¸ë£¹: {_gval or '-'}")

GROUP_INPUT_COUNTER = 0  # ê³ ì •: ì¹´ìš´í„°
CURRENT_GROUP_KEY = None  # í˜„ì¬ í”„ë ˆì„ì—ì„œ ì‚¬ìš©í•  user_key

# Keep only camera handle in session_state
if "cap" not in st.session_state:
    st.session_state.cap = None


def open_camera(index: int, target_w: int):
    cap = cv2.VideoCapture(index)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, target_w)
    return cap


# ENROLL UI lifecycle flags & unique keys
ENROLL_UI_BUILT = False
enroll_face_ph = None
enroll_form_counter = 0
current_enroll_form_key = None
current_enroll_name_key = None

# WELCOME ê·¸ë£¹ ì…ë ¥ìš© ê³ ìœ  í‚¤
WELCOME_KEY = None
WELCOME_KEY_COUNTER = 0

# WELCOME UI 1íšŒ ìƒì„± ê°€ë“œ & í”Œë ˆì´ìŠ¤í™€ë”
WELCOME_UI_BUILT = False
welcome_group_ph = None
welcome_status_ph = None
welcome_progress_ph = None

# Initial state
state = State.IDLE


# st.caption("Starting state machine...")


# ENROLL submit helper (ì´ë¦„ë§Œ ì €ì¥)
def ui_enroll_submit(new_name: str):
    global ENROLL_SUCCESS, USER_EXIST, name_list, embeddings, sh_current_user

    if not new_name or new_name.strip() == "":
        st.warning("ì´ë¦„ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
        return
    if sh_embedding is None or len(sh_embedding) == 0:
        st.error("ì–¼êµ´ ì„ë² ë”©ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ì— ì–¼êµ´ì„ ë˜‘ë°”ë¡œ ë¹„ì¶°ì£¼ì„¸ìš”.")
        return
    if any(n == new_name for n in name_list):
        st.warning("ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì´ë¦„ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.")
        return

    name_list.append(new_name)
    if embeddings.size:
        embeddings = np.vstack([embeddings, sh_embedding])
    else:
        embeddings = np.array([sh_embedding])

    save_db(name_list, embeddings)

    sh_current_user = new_name
    ENROLL_SUCCESS = True
    USER_EXIST = True

    st.success(f"ë“±ë¡ ì™„ë£Œ: {new_name}")
    print("[DB Updated] ", name_list, embeddings.shape)


# UI render helper
def render_state_panel(current_state: State):
    global ENROLL_UI_BUILT, enroll_face_ph, current_enroll_name_key, current_enroll_form_key
    global current_usercheck_key, usercheck_form_counter, sh_session_group  # â† ì •ë¦¬

    state_badge.markdown(f"**Current State:** :blue[{current_state.name}]")

    # ìƒíƒœë³„ ìŠ¬ë¡¯ ì •ë¦¬
    if current_state != State.USER_CHECK:
        if usercheck_slot is not None:
            usercheck_slot.empty()

    if current_state != State.ENROLL:
        enroll_slot.empty()
    if current_state != State.WELCOME:
        welcome_slot.empty()
    if current_state != State.ASR:
        asr_slot.empty()
    if current_state != State.BYE:
        bye_slot.empty()

    message_slot.markdown(f"**Message:** {sh_message}")

    # ---------- USER_CHECK ----------
    if current_state == State.USER_CHECK:
        usercheck_slot.empty()
        with usercheck_slot.container():
            st.info("ì‚¬ìš©ì í™•ì¸ ì¤‘ì…ë‹ˆë‹¤. ìœ„ì˜ â€˜ê·¸ë£¹ëª…â€™ ì…ë ¥ì°½ì— ì…ë ¥í•˜ì„¸ìš”. (BYE ì „ê¹Œì§€ ìœ ì§€)")

    # ---------- ENROLL ----------
    if current_state == State.ENROLL:
        if current_enroll_form_key is None or current_enroll_name_key is None:
            ts = int(time.time() * 1000)
            current_enroll_form_key = f"form_enroll_{ts}"
            current_enroll_name_key = f"enroll_name_{ts}"

        if not ENROLL_UI_BUILT:
            ENROLL_UI_BUILT = True
            with enroll_slot.container():
                st.info("ì•Œ ìˆ˜ ì—†ëŠ” ì‚¬ìš©ìì…ë‹ˆë‹¤. ì•„ë˜ í¼ìœ¼ë¡œ ë“±ë¡ì„ ì§„í–‰í•˜ì„¸ìš”.")
                globals()['enroll_face_ph'] = st.empty()
                with st.form(key=current_enroll_form_key, clear_on_submit=False):
                    new_name = st.text_input("ì´ë¦„", key=current_enroll_name_key)
                    submitted = st.form_submit_button("ë“±ë¡í•˜ê¸°", use_container_width=True)
                if submitted:
                    ui_enroll_submit(new_name)

        if enroll_face_ph is not None:
            if sh_face_crop is not None and sh_face_crop.size != 0:
                face_rgb = cv2.cvtColor(sh_face_crop, cv2.COLOR_BGR2RGB)
                enroll_face_ph.image(face_rgb, caption="ë“±ë¡í•  ì–¼êµ´", use_container_width=True)
            else:
                enroll_face_ph.warning("ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ë¥¼ í–¥í•´ í•œ ëª…ë§Œ ë¹„ì¶°ì£¼ì„¸ìš”.")
    else:
        if ENROLL_UI_BUILT:
            ENROLL_UI_BUILT = False
            globals()['enroll_face_ph'] = None

    # ---------- WELCOME ----------
    if current_state == State.WELCOME:
        welcome_slot.empty()
        with welcome_slot.container():
            if sh_session_group:
                st.caption(f"(ì„¸ì…˜ ê·¸ë£¹: {sh_session_group})")
            if not (time.time() > sh_timer_end):
                remain = max(0.0, sh_timer_end - time.time())
                st.success(f"Hi, **{sh_current_user}**! ê³§ ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                st.progress(min(max(1.0 - (remain / 2.0), 0.0), 1.0), text="Greeting...")
            else:
                if VAD_TASK_RUNNING:
                    st.info("ğŸ™ï¸ ìŒì„± ë…¹ìŒ ì¤‘...")
                elif VAD:
                    st.success("ğŸ§ ìŒì„± ìº¡ì²˜ ì™„ë£Œ! ASRë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                else:
                    st.warning("ë…¹ìŒì„ ì‹œì‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëŒì•„ê°‘ë‹ˆë‹¤.")

    # ---------- ASR ----------
    if current_state == State.ASR:
        asr_slot.empty()
        with asr_slot.container():
            if ASR_TASK_RUNNING:
                st.info("ğŸ§  Whisperë¡œ ìŒì„±ì„ ë³€í™˜ ì¤‘...")
            elif ASR_TEXT is not None:
                st.write("**ASR ê²°ê³¼:** ", ASR_TEXT)
                st.write(f"**BYE detected:** {'Yes' if BYE_EXIST else 'No'}")
                if sh_tts_file:
                    audio_slot.audio(sh_tts_file)
            else:
                st.write("ëŒ€ê¸° ì¤‘...")

    # ---------- BYE ----------
    if current_state == State.BYE:
        bye_slot.empty()
        with bye_slot.container():
            st.warning(f"Bye, **{sh_current_user}**!")
            if sh_session_group:
                st.caption(f"(ì„¸ì…˜ ê·¸ë£¹: {sh_session_group})")
            remain = max(0.0, sh_timer_end - time.time())
            pct = min(max(1.0 - (remain / 2.0), 0.0), 1.0)
            st.progress(pct, text="Ending...")


# ========= run =========
if run:
    # camera open
    if st.session_state.cap is None or not st.session_state.cap.isOpened():
        st.session_state.cap = open_camera(int(cam_index), int(width))
        if not st.session_state.cap.isOpened():
            st.error("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ë¥¼ ë°”ê¾¸ê±°ë‚˜ ë‹¤ë¥¸ ì•±ì„ ì¢…ë£Œí•´ë³´ì„¸ìš”.")
            st.stop()

    # initial state
    state = State.IDLE

    frame_i = 0
    # main loop
    while run:
        t0 = time.time()
        frame_i += 1

        # update bbox smoothing window
        BBOX_AVG_N = int(bbox_avg_n_ui)  # globally referenced in update_face_detection()

        success, sh_frame = st.session_state.cap.read()
        if not success:
            st.error("í”„ë ˆì„ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            break

        # key = cv2.waitKey(1) & 0xFF
        key = None

        _gkey = st.session_state.current_group_key
        sh_session_group = (st.session_state.get(_gkey, "") or "").strip() or None

        # state call + transition
        call_state_fn(state, key)
        new_state = state_transition(state)

        if new_state != state:
            print(f"State Change: {state.name} -> {new_state.name}")

            # ENROLLë¡œ ì§„ì… ì‹œ: ì´ˆê¸°í™” ë° í¼ í‚¤ ì„¤ì •
            if new_state == State.ENROLL and state != State.ENROLL:
                ENROLL_SUCCESS = False
                USER_EXIST = False
                ENROLL_UI_BUILT = False
                enroll_form_counter += 1
                current_enroll_form_key = f"form_enroll_{enroll_form_counter}"
                current_enroll_name_key = f"enroll_name_{enroll_form_counter}"  # â† ì¶”ê°€

            # WELCOMEë¡œ ì§„ì… ì‹œ: íƒ€ì´ë¨¸/ë…¹ìŒ í”Œë˜ê·¸ ì´ˆê¸°í™” + ì„¸ì…˜ ê·¸ë£¹ ì´ˆê¸°í™”
            if new_state == State.WELCOME:
                sh_timer_end = time.time() + 2.0
                VAD = False
                VAD_TASK_STARTED = False
                VAD_TASK_RUNNING = False
                sh_audio_file = None
                sh_tts_file = None

                SESSION_USER = sh_current_user
                USER_SWITCHED = False

                # sh_session_group = None

                # # ğŸ”‘ ì´ë²ˆ ë°©ë¬¸ìš© ê³ ìœ  key ìƒì„±
                # WELCOME_KEY_COUNTER += 1
                # WELCOME_KEY = f"welcome_group_input_{WELCOME_KEY_COUNTER}"
                #
                # # ì˜ˆì „ welcome_group_input_* í‚¤ ì œê±°
                # for k in list(st.session_state.keys()):
                #     if k.startswith("welcome_group_input_") and k != WELCOME_KEY:
                #         st.session_state.pop(k, None)
                #
                # # ë‹¤ìŒ ì¤„ì€ ìƒˆë¡œ ì¶”ê°€: ì´ë²ˆ ë°©ë¬¸ UIë¥¼ ë‹¤ì‹œ ë§Œë“¤ ìˆ˜ ìˆê²Œ ë¦¬ì…‹
                # WELCOME_UI_BUILT = False

            # ASRë¡œ ì§„ì… ì‹œ: ASR ë¹„ë™ê¸° ì´ˆê¸°í™”
            if new_state == State.ASR:
                ASR_TEXT = None
                BYE_EXIST = False
                ASR_TASK_STARTED = False
                ASR_TASK_RUNNING = False
                # ê·¸ë£¹ì€ ìœ ì§€ (BYEê¹Œì§€)

            # BYEë¡œ ì§„ì… ì‹œ: íƒ€ì´ë¨¸
            if new_state == State.BYE:
                sh_timer_end = time.time() + 2.0

            # BYE -> IDLEë¡œ ë– ë‚  ë•Œ: ì„¸ì…˜ ê·¸ë£¹/í‚¤ ì •ë¦¬
            if state == State.BYE and new_state == State.IDLE:
                sh_session_group = None

                SESSION_USER = None
                USER_SWITCHED = False

                # ìƒˆ í‚¤ ë¯¸ë¦¬ ë§Œë“¤ê¸°
                new_key = f"usercheck_group_input_{st.session_state.group_key_counter + 1}"

                # ì˜ˆì „ ê·¸ë£¹ ìœ„ì ¯ í‚¤ë“¤ ì •ë¦¬ (ìƒˆ í‚¤ëŠ” ìœ ì§€)
                for k in list(st.session_state.keys()):
                    if k.startswith("usercheck_group_input_") and k != new_key:
                        st.session_state.pop(k, None)

                # ì¹´ìš´í„°+í˜„ì¬í‚¤ ê°±ì‹ 
                st.session_state.group_key_counter += 1
                st.session_state.current_group_key = new_key

                # ìƒˆ í‚¤ë¡œ ì…ë ¥ì°½ì„ ë‹¤ì‹œ ë§Œë“¤ê¸° ìœ„í•´ rerun
                st.rerun()

            state = new_state

        # UI panel
        render_state_panel(state)

        # draw overlays
        display_frame = sh_frame
        if sh_bbox:
            x, y, w, h = sh_bbox
            cv2.rectangle(display_frame, (x, y), (x + w, y + h), sh_color, 2)
            cv2.putText(display_frame, sh_message, (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, sh_color, 2)
        else:
            cv2.putText(display_frame, sh_message, (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, sh_color, 2)

        frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
        h0, w0, _ = display_frame.shape
        new_h = int(h0 * (width / w0))
        resized = cv2.resize(display_frame, (int(width), new_h))
        frame_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)

        frame_slot.image(
                frame_rgb,
                channels="RGB",
                caption="Live",
                use_container_width=True,
                output_format="JPEG",
        )
        del resized, frame_rgb, display_frame

        # debug info
        debug_ph.json({
                "FACE_DETECTED"     : FACE_DETECTED,
                "USER_EXIST"        : USER_EXIST,
                "ENROLL_SUCCESS"    : ENROLL_SUCCESS,
                "VAD"               : VAD,
                "VAD_TASK_RUNNING"  : VAD_TASK_RUNNING,
                "ASR_TASK_RUNNING"  : ASR_TASK_RUNNING,
                "BYE_EXIST"         : BYE_EXIST,
                "TIMER_EXPIRED"     : TIMER_EXPIRED,
                "current_user"      : sh_current_user,
                "session_group"     : sh_session_group,  # â† ì„¸ì…˜ í•œì • ê·¸ë£¹ í‘œì‹œ (DB ì €ì¥ ì•ˆ í•¨)
                "audio_file"        : sh_audio_file,
                "tts_file"          : sh_tts_file,
                "bbox_avg_n"        : BBOX_AVG_N,
                "len(_bbox_history)": len(_bbox_history),
                "id(whisper_model)" : id(whisper_model),
                "id(resnet)"        : id(resnet),
                "id(face_detection)": id(face_detection),
        })

        if frame_i % 30 == 0:
            gc.collect()
            frame_i = 0
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        # FPS ì œí•œ (ì˜ˆ: 30fps)
        elapsed = time.time() - t0
        sleep_for = max(0.0, (1 / 5) - elapsed)
        time.sleep(sleep_for)
        run = st.session_state.get("_toggle_run", True)

    # cleanup
    if st.session_state.cap is not None:
        st.session_state.cap.release()
        st.session_state.cap = None
    cv2.destroyAllWindows()
    frame_slot.empty()
    st.info("ì¹´ë©”ë¼ë¥¼ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
